###################################################################################################
#' mk_test
#'
#' @description
#' mk_test performs a Mann-Kendall Test and return multiple test statistics given a
#' variable y and a variable x in vector form.
#'
#' @param y_var a vector of numerics representing the y variable
#' @param x_var a vector of numerics representing the x variable
#' @param z_low a numeric representing the z-value for the lower confidence level,
#' defaulted to 0.52 representing the 70 percent confidence threshold
#' @param z_high a numeric representing the z-value for the upper confidence level,
#' defaulted to 1.28 representing the 90 percent confidence threshold
#' @param keep_z a boolean for whether or not z-value will be included,
#' defaulted to FALSE
#'
#' @return a dataframe of the form || slope || intercept || ConfidenceLevel || z_value ||
#'
#' @details
#'1. y and x variables should have equal length
#'2. Although the function does allow input y to contain duplicates of a certain value,
#'   please take into account that an excess amount of duplicates within y will likely
#'   decrease the accuracy of the result generated by the Mann-Kendall test. For data
#'   set with too many ties in the dependant variable, consider testing for the trend
#'   using alternative methods, such as generalized linear model. Refer to other functions
#'   such as hurdle_test and negbin below for more information.
#'
#' @export
mk_test <-function(y_var, x_var, z_low=0.52, z_high=1.28, keep_z=FALSE){
  sen <- zyp.sen(y_var~x_var)
  slope <- round(sen$coefficients[[2]],2)
  intercept <- round(sen$coefficients[[1]],2)
  corr<- cor.test(y_var, x_var, method = "kendall", alternative = "two.sided", exact = FALSE)
  Z <-abs(corr$statistic[["z"]])
  CATTrend <- case_when(Z>=z_high ~ "Confident",
                        Z>=z_low & Z<z_high ~ "Likely",
                        Z<z_low ~ "Uncertain")
  ret<- data.frame(slope=slope,intercept=intercept,CATTrend=CATTrend,z_value=Z)
  if(!keep_z){
    ret <- ret %>% dplyr::select(-z_value)
  }
  return(ret)
}

###################################################################################################
#' hurdle_test
#'
#' @description
#' hurdle_test performs a hurdle test and returns multiple test statstics given a
#' variable y and a variable x. Hurdle test can be used to fit a linear model when
#' a data set contains excess amount of zeros.
#'
#' @param y_var a vector of numeric representing the y variable
#' @param x_var a vector of numeric representing the x variable
#' @param zero_fam a character representing the model family for the zero portion
#' of the data. Defaulted to "binomial" indicating binomial family
#' @param count_dist a character representing the distribution to the count distribution.
#' Defaulted to 'negbin' indicating the negative binomial distribution
#' @param zero_dist a character representing the distribution to the zero distribution.
#' Defaulted to "binomial" indicating the binomial distribution
#' @param link a character representing the link function. Defaulted to
#' 'logit' meaning logarithm link function
#'
#' @return a dataframe of the form || slope || intercept || ConfidenceLevel ||,
#' or a character if the test is not applicable;
#'
#' @details
#' The hurdle test deal with zero and non-zero portion separatly using two different models,
#' using negative binomial distribution for the non-zero portion and binomial distribution
#' for the zero portion. Confident intervals are created correspondingly to measure the
#' level of confidence against the trend. A glm will be then established with a logarithm
#' link function. The slope and intercept of the resulting model will be recorded and returned.
#' Things to take notice of:
#' 1. y and x need to have the same length
#' 2. Although the detail section only describes the condition with negbin count distribution
#'    binomial zero distribution, this function can be used for other distributions as well.
#'    Do note that this function does not checks for the accuracy of the fitted model,
#'    neither does it provide any recommandations to what distribution fits the best.
#'    In order to use other families, it is highly suggested to look at the  concept of general
#'    linear beforehands.
#' 3. The program will return an error if the x variable does not contain any zeros.
#'    In order to calculate the trend using this function, do make sure that there is zero
#'    values in the x variable. If the error message "Data cannot be fitted using hurdle model"
#'    appears, user might want to consider using plain negative binomial model(provided in
#'    the negbin() function in the same package) for cases with excess amount of zeros,
#'    or Mann-Kendall Test for cases with limited ties and zeros.
#' @export
hurdle_test <- function(y_var,x_var,zero_fam="binomial",count_dist="negbin",zero_dist="binomial",link="logit"){
  # Are we confident there is a trend?
  # Count portion of hurdle
  y_new <-y_var[y_var>0]
  model.count <- tryCatch(MASS::glm.nb(y_new~x_var),
                          error=function(e){return("A")})
  low.c<- ifelse(is.character(model.count), NA, exp(confint(model.count, level=0.9))[2,1])
  hi.c <- ifelse(is.character(model.count), NA, exp(confint(model.count, level=0.9))[2,2])
  # Zero portion of hurdle
  zero_indicator <- !I(y_var==0)
  model.zero <- tryCatch(glm(zero_indicator~x_var, family=zero_fam),
                         error=function(e){return("A")})
  low.z<-ifelse(is.character(model.zero), NA, exp(confint(model.zero, level=0.9))[2,1])
  hi.z<-ifelse(is.character(model.zero), NA, exp(confint(model.zero, level=0.9))[2,2])
  if(!is.na(low.c)&!is.na(hi.c)&!is.na(low.z)&!is.na(hi.z)){
    # combined confidence test @ 70% and 90% confidence
    pass <- ifelse(all(low.c*low.z<=1, hi.c*hi.z>=1), "Maybe?", "Confident")
    if (pass == "Maybe?"){
      low.c<- exp(confint(model.count, level=0.7))[2,1]
      hi.c <- exp(confint(model.count, level=0.7))[2,2]
      low.z<-exp(confint(model.zero, level=0.7))[2,1]
      hi.z<-exp(confint(model.zero, level=0.7))[2,2]
      pass <- ifelse(all(low.c*low.z<=1, hi.c*hi.z>=1), "Uncertain", "Likely")
    }
    # Get slope and intercept from hurdle
    model <- hurdle(y_var~x_var, dist=count_dist, zero.dist=zero_dist, link=link)
    fitted <- unname(model$fitted.values)
    slope <- round((fitted[length(fitted)] - fitted[1])/
                     (max(data.p$year) - min(data.p$year)),2)
    intercept < - round((fitted[1]-min(data.p$year)*((fitted[length(fitted)] - fitted[1])/
                                                       (max(data.p$year) - min(data.p$year)))),2)
    data.frame(slope=slope,intercept=intercept,CATTrend=pass)
  }else{
    "Error: Data cannot be fitted using hurdle model"
  }
}

###################################################################################################
#' negbin
#'
#' @description
#' negbin fits a negative binomial model and return multiple test statstics given a
#' variable y and a variable x.
#'
#' @param y_var a vector of numeric representing the y variable
#' @param x_var a vector of numeric representing the x variable
#'
#' @return a dataframe of the form || slope || intercept || ConfidenceLevel ||
#'
#' @details
#' The negbin function fits a negative binomial model to a sets of x and y variables
#' Confident intervals are created correspondingly to measure the
#' level of confidence against the trend. The slope and intercept of the resulting model will be
#' recorded and returned.
#' Things to take notice:
#' 1. y and x need to have the same length
#' 2. Theoretically saying, negbin is the most versatile among the three trend tests.
#'    The preferred condition to use negbin function is when there are neither an excess amount
#'    of zeros, nor can the trend be calculated by Mann-Kendall test. Under these conditions,
#'    consider using the two other functions instead.
#' @export
negbin <- function(y_var, x_var){
  model <- tryCatch(glm.nb(y_var~x_var,
                           error=function(e){return("A")}))
  low<-ifelse(is.character(model), NA, exp(confint(model, level=0.9))[2,1])
  hi<-ifelse(is.character(model), NA, exp(confint(model, level=0.9))[2,2])
  if (!is.na(low)&!is.na(hi)){
    # confidence test @ 70% and 90% confidence
    pass <- ifelse(all(low<=1, hi>=1), "Maybe?", "Confident")
    if (pass=="Maybe?"){
      low<- exp(confint(model, level=0.7))[2,1]
      hi<-exp(confint(model, level=0.7))[2,2]
      pass <- ifelse(all(low<=1, hi>=1),"Uncertain", "Likely")
    }
    # Get slope from negative binomial
    fitted <- unname(model$fitted.values)
    slope <- round((fitted[length(fitted)] - fitted[1])/
                     (max(data.p$year) - min(data.p$year)),2)
    intercept <- round((fitted[1]-min(data.p$year)*((fitted[length(fitted)] - fitted[1])/
                                                      (max(data.p$year) - min(data.p$year)))),2)
    data.frame(slope=slope,intercept=intercept,CATTrend=pass)
  }else{
    "Error: Data cannot be fitted using negative binomial model"
  }
}

###################################################################################################
#' bday_calculation
#'
#' @description
#' bday_calculation calculates the last and first bday for some years with
#' record at a station.
#'
#' @param station a character representing the station number that exists in hydat
#' @param year a vectoer of numeric representing the list of years used for calculating
#' the threshold
#'
#' @return a dataframe of the format || station || year || lastbday || firstbday ||
#' @export
bday_calculation <- function(station, year="all"){
  flow.daily <- hy_daily_flows(station)
  flow.daily$Year <- as.numeric(format(flow.daily$Date, "%Y"))
  flow.daily$Month<- as.numeric(format(flow.daily$Date, "%m"))
  if (year=="all"){
    valid_year=unique(flow.daily$Year)
  }else if(any(!year %in% flow.daily$Year)){
    return("Some of the year provided are not in HYDAT")
  }else{
    valid_year=year
  }
  first_vec = c()
  last_vec = c()
  for (i in valid_year){
    print(i)
    flow.late <- flow.daily %>% filter(Year==i & Month > 8)
    firstbday <- flow.late %>% filter(Symbol == "B", Month >= 10) %>% head(1)
    firstbday.date <- ifelse(nrow(firstbday)>0, substring(firstbday$Date,1,10), NA)

    firstbday.long <- ifelse(is.na(firstbday.date),paste0(i,"-10-01"),
                             ifelse(firstbday.date > as.Date(paste0(i, "-10-01")),
                                    paste0(i, "-10-01"),firstbday.date))
    first_vec = c(first_vec, firstbday.long)
    # Last B day
    flow.early <- flow.daily %>% filter(Year == i & Month < 6)
    lastbday <- flow.early %>% filter(Symbol == "B", Month < 6) %>% tail(1)
    lastbday.date <- ifelse(nrow(lastbday)>0, substring(lastbday$Date,1,10), NA)

    lastbday.long<- ifelse(is.na(lastbday.date),paste0(i,"-05-01"),
                           ifelse(lastbday.date < as.Date(paste0(i, "-05-01")),
                                  paste0(i,"-05-01"),lastbday.date))
    last_vec = c(last_vec, lastbday.long)
  }
  ret = data.frame(Station=rep(station, length(valid_year)),
                   Year = valid_year,
                   LastBday = last_vec,
                   FirstBday = first_vec)
  return(ret)
}

###################################################################################################
#' Threshold_Build
#'
#' @description
#' Threshold_Building takes into a list of stations, year_from and year_to representing the beginning
#' and the end of the reference period, and the two ratio numbers hi and low representing the
#' percentile(high and low). bday_calculation function will be needed to calculate the threshold.
#'
#'
#' @param stations a vector of characters indicating the list of station we want to calculate the
#' threshold on;
#' @param yfrom a numeric indicating the year threshold calculation starts, defaulted to 1981;
#' @param yto a numeric indicating the year threshold calculation ends, defaulted to 2010;
#' @param hi a numeric indicating the upper confident threshold, defaulted to 0.95;
#' @param low a numeric indicating the lower confident threshold, defaulted to 0.05
#' @param write_csv a boolean value indicating whether ot not the output will be written
#' to csv, defaulted to FALSE;
#'
#' @return
#' A dtaaframe of the format:
#' || station ||UpperThreshold ||LowerThreshold ||
#'
#' @export

Threshold_Build <- function(stations, yfrom=1981, yto=2010, hi=0.95, low=0.05, write_csv=FALSE){
  Upper = c()
  Lower = c()
  for (i in 1:length(stations)){
    stn.id <- stations[i]
    print(stn.id)
    flow.daily <- hy_daily_flows(stn.id)
    flow.daily$Year = substr(flow.daily$Date, 1, 4)
    ref <- flow.daily %>% filter(Year>=yfrom, Year<=yto)
    summ = ref %>% group_by(Year) %>% summarise(count=length(Value[!is.na(Value)]))
    if (length(summ$Year)<20 | length(summ$count[summ$count>=150])<length(summ$count)){
      Upper=append(Upper, NA)
      Lower=append(Lower, NA)
      print(paste0(stn.id, "---not enough data"))
    }else{
      Upper_All=ref$Value
      for (i in 1:length(Upper_All)){
        if (is.na(Upper_All[i])){
          Upper_All[i]=0
        }
      }
      yr=summ$Year
      num_leap_yrs=length(yr[as.numeric(yr)%%4==0])
      total_length=length(yr)*365+num_leap_yrs
      Upper_All=append(Upper_All, rep.int(0, total_length-length(Upper_All)))
      Lower_All=c()
      # First B day
      flow.daily$Month = as.numeric(format(flow.daily$Date, "%m"))
      for (j in 1:length(summ$Year)){
        year.op=summ$Year[j]
        fbday=bday_calculation(stn.id, year.op)
        lastbday = (fbday %>% filter(Year==year.op))[["LastBday"]]
        firstbday = (fbday %>% filter(Year==year.op))[["FirstBday"]]
        Value_within=(ref %>% filter(as.Date(Date)>=as.Date(lastbday)&as.Date(Date)<=as.Date(firstbday)))$Value
        Value_within=Value_within[Value_within!=0]
        Lower_All=append(Lower_All, Value_within)
      }
      Upper=append(Upper,quantile(Upper_All, probs = hi, names = FALSE, na.rm = TRUE))
      Lower=append(Lower,quantile(Lower_All, probs = low, names = FALSE, na.rm = TRUE))
      print(stn.id)
    }
  }
  Threshold = data.frame(STATION_NUMBER = stations, Q_Upper=Upper, Q_Lower=Lower)
  if (write_csv){
    write.csv(Threshold, file = "../Dependencies/Threshold_New.csv", row.names = FALSE)
  }
  return(Threshold)
}
###################################################################################################
#' percentile.ranked
#' @description
#' Calculates the pencentage of values in a vector smaller than value
#' @param a.vector a vector of any comparable data type, ideally numeric;
#'
#' @param value a comparable data type to the components in a.vector, ideally numeric
#'
#' @return the percentage of values within the vector that's smaller than the value itself.
#' @export
percentile.ranked <- function(a.vector, value) {
  numerator <- length(sort(a.vector)[a.vector < value])
  denominator <- length(a.vector)
  round(numerator/denominator,2)*100
}


###################################################################################################
#' Get_ann_mean_flow
#'
#' @description
#' Get_ann_mean_flow calculates the annual mean flow based on a station and a year
#' vector indicating which year the mean flow will be calculated.
#'
#' @param station a character indicating the station number to calculate the annual mean flow;
#' @param year a character indicating the list of years where the annual mean flow is being
#' calculated, or by default equals to 'all' indicating all recorded years at the station
#'
#' @return
#' a data frame of the format:
#' || station || Year || annual mean yield||
#'
#' @export

Get_ann_mean_flow <- function(station, year='all'){
  ret=data.frame()
  mean_yield = c()
  flow.daily <- hy_daily_flows(station)
  flow.daily$Year <- as.numeric(format(flow.daily$Date, "%Y"))
  if (year=="all"){
    valid_year=unique(flow.daily$Year)
  }else if(any(!year %in% flow.daily$Year)){
    return("Some of the year provided are not in HYDAT")
  }else{
    valid_year=year
  }
  for (i in valid_year){
    flow.dates = flow.daily[flow.daily$Year==i,]
    if (nrow(flow.dates %>% filter(!is.na(Value)))<150){
      ann_mean_yield = NA
    }else{
      if (nrow(flow.dates%>%filter(!is.na(Value))) > 0.9 * ifelse(flow.dates$Year[1]%%4==0, 366, 365)){
        ann_mean_yield <- flow.dates %>% mutate(mean_flow = mean(Value, na.rm = TRUE))
        ann_mean_yield <- ann_mean_yield[1, "mean_flow"] %>% round(2)
        ann_mean_yield <- ann_mean_yield$mean_flow
      } else {
        Mode <- flow.daily  %>% group_by(Year) %>% filter(!is.na(Value)) %>%
          summarise(MODE=names(sort(table(Value), decreasing = TRUE)[1]))
        Mode.multi <- mean(as.numeric(Mode$MODE))
        if (Mode.multi <0.01){
          ann_mean_yield <- flow.dates %>% filter(!is.na(Value)) %>% group_by(Year) %>%
            summarise(TOTAL=sum(Value)/n())
          ann_mean_yield <- round(ann_mean_yield$TOTAL,2)
        } else {
          ann_mean_yield <- NA
        }
      }
    }
    mean_yield <- c(mean_yield, ann_mean_yield)
  }
  print(1)
  ret = data.frame(STATION_NUMBER=rep(station, length(mean_yield)),
                   Year = valid_year,
                   ann_mean_yield = mean_yield)
  return(ret)
}

#############################################
#' Get_flood_metrics
#'
#' @description
#' Get_flood_metrics takes in a character representing the station number within 1014
#' RHBN_U stations and a vector of numerics or a character, calculates four flood metrics:
#' pot_threshold: the upper threshold that was calculated in Threshold_build function from above;
#' pot_days:days over the threshold within a year;
#' pot_events:independent flood events over a year;
#' pot_ma_dur: the maximum consecutive duration over the threshold within a year;
#' The four metrics will then be recorded in a table and returned.
#'
#' @details
#' Note that there are requirements regarding the independence conditions for pot_event metric,
#' we implement a check for the correlation between flood events.
#' Referring to Lang etal (1999), we set two conditions:
#' 1) R > 5+log(A/1.609^2);
#' 2) Xmin < 0.75 * min(Xi, Xj);
#' where R is the time in days between two peaks, A is the watershed area in squared kilometers;
#' Xmin is the minimum flow between the adjacent flow peaks Xi and Xj.
#' In order for two seperate peaks to be considered as independent events of drought or flood,
#' both conditions need to be met. If either of the conditions fail, we consider two events
#' as the same one. R is defined within the iteration.
#'
#' @references
#' Slater et al.(2016). On the impact of gaps on trend detection un extreme
#' streamflow time series. Int.J.climatol. 37:3976-3983(2017). doi: 10.1002/joc.4954
#'
#' @param station a character representing a station we want the flood metrics to be calculated at;
#' @param year a vector of numeric representing the list of years we want thew flood metrics to be calculated at,
#' defaulted to 'all' indicating all the years at the station;
#'
#' @return
#' A dataframe of the format:
#' || STATION_NUMBER || Year || pot_days || pot_events || pot_threshold || pot_max_dur ||
#'
#' @export
Get_flood_metrics <- function(station, year='all'){
  ret = data.frame()
  flow.daily <- hy_daily_flows(station)
  flow.daily$Year <- as.numeric(format(flow.daily$Date, "%Y"))
  if (year=="all"){
    valid_year=unique(flow.daily$Year)
  }else if(any(!year %in% unique(flow.daily$Year))){
    return("Some of the year provided are not in HYDAT")
  }else{
    valid_year=year
  }
  station_row = Threshold_Build(station)
  pot_days = c()
  pot_events = c()
  pot_threshold = c()
  pot_max_dur = c()
  stations <- read.csv("../Dependencies/RHBN_U.csv", header = TRUE)
  stn_area <- stations$Shp_Area[stations$STATION_NUMBER == station]
  R <- 5 + log(stn_area/(1.609^2))%>%round(1)
  for (yr in valid_year){
    flow.dates <- flow.daily %>% filter(Year == yr)
    max.flow <- flow.dates %>% arrange(desc(Value)) %>% slice(1)
    max.flow <- round(max.flow[["Value"]], digits = 2)
    if (is.na(station_row$Q_Upper)){
      pot_days = c(pot_days, NA)
      pot_events =c(pot_events, NA)
      pot_threshold = c(pot_threshold, NA)
      pot_max_dur = c(pot_max_dur, NA)
    }else{
      threshold <- as.numeric(station_row$Q_Upper)
      # POT function triggers 'computational singularity' warning or error when threshold >=10
      # This factor scales inputs and one of the outputs below
      if (max.flow<threshold){
        pot_days = c(pot_days, 0)
        pot_events =c(pot_events, 0)
        pot_threshold = c(pot_threshold, threshold)
        pot_max_dur = c(pot_max_dur, 0)
      }else{
        factor <- case_when( threshold >= 1000 ~ 1000,
                             threshold >= 100 ~ 100,
                             threshold >= 10 ~ 10,
                             threshold >= 0 ~ 1)
        pot2 <- possibly(evir::pot, otherwise = NA)
        pot <- pot2(flow.dates$Value[!is.na(flow.dates$Value)]/factor, threshold/factor)
        # here is the singularity warning!

        if (!is.list(pot)) {
          pot_days = c(pot_days, NA)
          pot_events =c(pot_events, NA)
          pot_threshold = c(pot_threshold, NA)
          pot_max_dur = c(pot_max_dur, NA)

        } else {
          pot_days =c(pot_days, length(pot$data))
          pot.dates <- data.frame(dates = attr(pot$data, "times"))
          pot.dates$lag <- pot.dates$dates - lag(pot.dates$dates, 1)
          pot_threshold = c(pot_threshold, threshold)
          seq_blocks <- split(pot.dates$dates, cumsum(c(TRUE, diff(pot.dates$dates)!=1)))
          # We iterate within seq_blocks to find maximum flow within each events, before testing
          # their independence
          events_count <- c()
          for (a in 1:length(seq_blocks)){
            events_count <- append(events_count, length(seq_blocks[[as.character(a)]]))
          }
          events_array <- c()
          for (b in 1:length(events_count)){
            c <- events_count[b]
            while (c > 0){
              events_array<-append(events_array,b)
              c <- c-1
            }
          }
          pot_dat <- data.frame(data = pot$data, dates=attr(pot$data, "times"),event_num=events_array)
          max_peak <- pot_dat %>% group_by(event_num) %>% summarise(max=max(data))
          max_peak_central<-c()
          for (d in 1:length(max_peak$max)){
            max_peak_dates<-(pot_dat %>% filter(data==max_peak$max[d],dates>=seq_blocks[[d]][1],
                                                dates<=seq_blocks[[d]][length(seq_blocks[[d]])]))$dates
            if (length(max_peak_dates)%%2==1){
              max_peak_dates=max_peak_dates[(length(max_peak_dates)+1)/2]
            } else {
              max_peak_dates=(max_peak_dates[length(max_peak_dates)/2]+
                                max_peak_dates[length(max_peak_dates)/2+1])/2
            }
            max_peak_central=append(max_peak_central, max_peak_dates)
            max_peak_dates=c()
          }
          max_peak$central_dates<-max_peak_central
          R_vec<-c()
          for (t in 1:length(max_peak$max)){
            R_vec<-append(R_vec, R)
          }
          max_peak$R_vec<-R_vec
          Xmin_vec<-c()
          if (length(max_peak$max)==1){
            Xmin_vec<-append(Xmin_vec, NA)
          }else{ for( e in 1:(length(max_peak$max)-1)){
            Xmin_vec<-append(Xmin_vec,
                             0.75*min(max_peak$max[e],max_peak$max[e+1])*factor)
          }
            Xmin_vec<-append(Xmin_vec, NA)
          }
          max_peak$Xmin_vec<-Xmin_vec
          ind_events<-c()
          if (length(seq_blocks)>1){
            for (j in 1:(length(seq_blocks)-1)){
              range_flow <- flow.dates %>% filter(Date>=as.Date(max_peak$central_dates[j], origin=paste0(yr, "-01-01")),
                                                  Date<=as.Date(max_peak$central_dates[j+1],origin=paste0(yr, "-01-01")))
              if (all(min(range_flow$Value, na.rm=TRUE)<Xmin_vec[j],
                      ((max_peak$central_dates[j+1] - max_peak$central_dates[j])>R))){
                ind_events <- append(ind_events, 1)
              }else{
                ind_events <- append(ind_events, 0)
              }
            }
            ind_events <- append(ind_events, 1)
          } else { ind_events <- c(1)}
          max_peak$ind_events<-ind_events
          pot_events <- c(pot_events, sum(ind_events))
          max_dur_count <- c()
          count<-0
          for (f in 1:length(ind_events)){
            if (ind_events[f]==1 | is.na(ind_events[f])){
              max_dur_count = append(max_dur_count, count + events_count[f])
              count = 0
            } else { # i.e. in_events[i] == 0
              count <- count + events_count[f]}
          }
          pot_max_dur  = c(pot_max_dur, max(max_dur_count))
        }
      }
    }
  }
  ret = data.frame(STATION_NUMBER=rep(station,length(valid_year)),
                   YEAR=valid_year,
                   pot_days=pot_days,
                   pot_threshold=pot_threshold,
                   pot_events=pot_events,
                   pot_max_dur=pot_max_dur)
  return(ret)
}

#############################################
#' Get_drought_metrics
#'
#' @description
#' The function takes in a character representing the station number within 1014
#' RHBN_U stations and a vector of numerics or a character, calculates four drought metrics:
#' dr_threshold: the lower threshold that was calculated in Threshold_build function from above;
#' dr_days:days under the lower threshold within a year;
#' dr_events:independent drought events over a year;
#' dut_ma_dur: the maximum consecutive duration under the threshold within a year;
#'
#' @details
#' The four metrics will then be calculated based on only the time within a year between the
#' last and first bday calculated from the previous bday_calculation() function,
#' and the result will be recorded in a table and returned.
#' In order to measure the independence between drought events, we set the following
#' constraints:
#' 1. t >= 5
#' 2. the excess volume, Vabove / min(Vbelowi, Vbelowj)>=0.1
#' where t is the time between events, Vabove is the amount that exceeds the threshold,
#' Vbelowi and Vbelowj are the volumes below during events i and j.
#'
#' @param station a character representing a station we want the flood metrics to be calculated at;
#' @param year a vector of numeric representing the list of years we want thew flood metrics to be calculated at,
#' defaulted to 'all' indicating all the years at the station;
#'
#' @references
#' WMO World Meteorological Organization (2008). Manual on Low-Flow Estimation and Prediction. 1339 Operational Hydrology.
#'
#' @return
#' A dataframe of the format:
#' || STATION_NUMBER || Year || dr_days || dr_events || dr_threshold || dut_max_dur ||
#'
#' @export
Get_Drought_metrics <- function(station, year='all'){
  ret = data.frame()
  flow.daily <- hy_daily_flows(station)
  flow.daily$Year <- as.numeric(format(flow.daily$Date, "%Y"))
  if (year=="all"){
    valid_year=unique(flow.daily$Year)
  }else if(any(!year %in% unique(flow.daily$Year))){
    return("Some of the year provided are not in HYDAT")
  }else{
    valid_year=year
  }
  station_row = Threshold_Build(station)
  bday=bday_calculation(station, valid_year)
  dr_days = c()
  dr_events = c()
  dr_threshold = c()
  dut_max_dur = c()
  for (yr in valid_year){
    bd=bday %>% filter(Year==yr)
    flow.dates = flow.daily %>% filter(Year == yr)
    dates_value=flow.dates$Value
    if (is.na(flow.dates$Value[1])){
      front_ind=0
      while(is.na(dates_value[1])){
        dates_value=dates_value[2:length(dates_value)]
        front_ind=front_ind+1
      }
      date_ref_front=flow.dates$Date[front_ind+1]
      flow.dates=flow.dates %>% filter(as.Date(Date)>=as.Date(date_ref_front))
    }
    if (is.na(dates_value[length(dates_value)])){
      end_ind=0
      while(is.na(dates_value[length(dates_value)])){
        dates_value=dates_value[1:(length(dates_value)-1)]
        end_ind=end_ind+1
      }
      date_ref_end=flow.dates$Date[length(flow.dates$Value)-end_ind]
      flow.dates=flow.dates %>% filter(as.Date(Date)<=as.Date(date_ref_end))
    }
    date_vec<-flow.dates$Date
    flow.new<-flow.dates%>%mutate(Date=seq(1, n()))%>%mutate(Value=approx(Date,Value,Date)$y)
    flow.new$Date<-date_vec
    zooflow <- zoo(flow.new$Value, flow.new$Date)
    if (length(zooflow) > 60){
      low <- as.numeric(station_row$Q_Lower)
      data.s <- data.frame(day= as.numeric(substring(flow.new[["Date"]], 9,10)),
                           month=as.numeric(substring(flow.new[["Date"]],6,7)),
                           year=yr,flow=flow.new$Value, hyear=yr,
                           baseflow=NA,
                           date=paste0(yr,"-",substring(flow.new[["Date"]],6,11)))

      data.s <- data.s %>% filter(as.Date(data.s$date) >= as.Date((bd[,3])[1]) &
                                    as.Date(data.s$date) <= as.Date((bd[,4])[1]))

      data.lf <- createlfobj(data.s)
      flowunit(data.lf) <- "m^3/s"
      drought <- find_droughts(data.lf, threshold = low)
      summary2 <- possibly(base::summary, otherwise = data.frame(event.no=NULL))
      duration<-summary2(drought)

      if (any(duration[1,"event.no"] == 0, nrow(duration)==0)){
        dr_days  = c(dr_days,0)
        dr_events  = c(dr_events, 0)
        dut_max_dur =c(dut_max_dur, 0)

      } else {
        dr_days <- c(dr_days,sum(duration$duration))
        vol<-c()
        for (i in 1:length(drought$def.increase)){
          vol<-append(vol, drought$def.increase[[i]])
        }
        drevent_count<-c()
        zero_count<-c()
        for ( i in 1:length(drought$event.no)){
          drevent_count<-append(drevent_count, drought$event.no[[i]])
          if (drought$event.no[[i]]==0){
            zero_count<-append(zero_count, i)
          }
        }
        ind<-0
        if (length(zero_count)==0){ #i event that last for the entire period
          dr_days=c(dr_days,duration$duration)
          dr_events=c(dr_events,1)
          dut_max_dur=c(dut_max_dur,duration$duration)
        } else{
          if (length(zero_count)==1){
            drevent_count[zero_count[1]]<-"0_1"
          } else{
            for ( i in 1:(length(zero_count)-1)){
              if ((zero_count[i]+1)==zero_count[i+1]){
                drevent_count[zero_count[i]]<-paste0(as.character(ind), "_", as.character(ind+1))
              } else{
                drevent_count[zero_count[i]]<-paste0(as.character(ind), "_", as.character(ind+1))
                ind<-ind+1
              }
              drevent_count[zero_count[length(zero_count)]]<-paste0(as.character(ind), "_", as.character(ind+1))
            }
          }
          dr_filter<-data.frame(volume=vol, dr_event_count=drevent_count)
          dr_summary<-dr_filter %>% group_by(dr_event_count) %>% summarise(sum=sum(volume))
          dr_ind_events<-c()
          if (nrow(duration)>1){
            for ( i in 1:(nrow(duration)-1)){
              t <- as.numeric(duration$start[i+1]-duration$end[i])
              v_above<-(dr_summary %>% filter(dr_event_count==paste0(as.character(i), "_",
                                                                     as.character(i+1))))$sum
              if (any(t>=5, abs(v_above/min(duration$volume[i], duration$volume[i+1]))>=0.1)){
                dr_ind_events<-append(dr_ind_events, 1)
              }else{
                dr_ind_events<-append(dr_ind_events, 0)
              }
            }
            dr_ind_events<-append(dr_ind_events, 1)
          }else{
            dr_ind_events<-c(1)
          }
          dr_events <- c(dr_events,sum(dr_ind_events))
          dr_max_dur_count <- c()
          dr_count<-0
          for (i in 1:length(dr_ind_events)){
            if (dr_ind_events[i]==1){
              dr_max_dur_count <- append(dr_max_dur_count, dr_count + duration$duration[i])
              dr_count <- 0
            } else { # i.e. in_events[i] == 0
              dr_count <- dr_count + duration$duration[i]}
          }
          dut_max_dur <- c(dut_max_dur,max(dr_max_dur_count))
        }
      }
    } else {
      dr_days <- c(dr_days,NA)
      dr_events <- c(dr_events,NA)
      dut_max_dur <- c(dut_max_dur,NA)

    }
    dr_threshold <- c(dr_threshold,low)
  }
  ret = data.frame(STATION_NUMBER=rep(station,length(valid_year)),
                   YEAR=valid_year,
                   dr_days=dr_days,
                   dr_threshold=dr_threshold,
                   dr_events=dr_events,
                   dut_max_dur=dut_max_dur)
  return(ret)
}

